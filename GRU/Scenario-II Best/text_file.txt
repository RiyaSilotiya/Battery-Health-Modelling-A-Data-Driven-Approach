Case3: One input feature allocated to each recurrent layer
		IF1=F1; IF2=F2;
		IF3=F3; IF4=F4

MSE=mean_squared_error(y_train,trainPredict)=0.001318536480168418
MSE=mean_squared_error(y_val,valPredict)=0.002684150138490828
MSE=mean_squared_error(y_test,testPredict)=0.0005144523578225843

RMSE = (y_train,trainPredict)= 0.04
RMSE = (y_val,valPredict)= 0.05
RMSE = (y_test,testPredict)= 0.02

MAE =(y_train,trainPredict)=0.17
MAE =(y_val, valPredict)=0.22
MAE =(y_test,testPredict)=0.14

r2_score(y_train, trainPredict)=0.9801043499884434
r2_score(y_val, valPredict)=0.9054122735273364
r2_score(y_test, testPredict)=0.9681678043597348
r2_score(combined_data, combined_datap)=0.9744808708575012






from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Dense,add,Lambda,Flatten

rnn_1 = GRU(units=11)(input_1)
dropout1 = Dropout(rate=0.1)(rnn_1)

rnn_2 = GRU(units=11)(input_2)
dropout2 = Dropout(rate=0.1)(rnn_2)

rnn_3 = GRU(units=11)(input_3)
dropout3 = Dropout(rate=0.1)(rnn_3)

rnn_4 = GRU(units=11)(input_4)
dropout4 = Dropout(rate=0.1)(rnn_4)

# Concatenate the outputs of the recurrent layers
merged = concatenate([dropout1, dropout2, dropout3, dropout4], axis=1)


flatten = Flatten()(merged)
activation_layer = Activation('tanh')(flatten)
dense1 = Dense(units=50, activation='tanh', kernel_regularizer=regularizers.l1(0.001))(activation_layer)
dense1 = BatchNormalization()(dense1)
dense2 = Dense(units=25, activation='relu', kernel_regularizer=regularizers.l1(0.001))(dense1)
dense2 = BatchNormalization()(dense2)

output = Dense(units=1, activation='sigmoid')(dense2)

regressor = Model(inputs=[input_1, input_2,input_3, input_4], outputs=output)

# Compile the model with the desired optimizer, loss function, and metrics
regressor.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# Train the model with the training data
result=regressor.fit([X_train1, X_train2,X_train3, X_train4], y_train, epochs=1000, batch_size=25,validation_data=([X_val1, X_val2,X_val3, X_val4], y_val),callbacks=[LearningRateLogger()])
