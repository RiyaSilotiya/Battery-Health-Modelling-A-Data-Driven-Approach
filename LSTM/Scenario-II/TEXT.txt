Case3: One input feature allocated to each recurrent layer
		IF1=F1; IF2=F2;
		IF3=F3; IF4=F4
		LSTM

MSE=mean_squared_error(y_train,trainPredict)=0.0017641701568284244
MSE=mean_squared_error(y_val,valPredict)=0.003428717039426336
MSE=mean_squared_error(y_test,testPredict)=0.0030662866305011995

RMSE = (y_train,trainPredict)=  0.04
RMSE = (y_val,valPredict)= 0.06
RMSE = (y_test,testPredict)= 0.06

MAE =(y_train,trainPredict)= 0.19
MAE =(y_val, valPredict)= 0.22
MAE =(y_test,testPredict)= 0.22

r2_score(y_train, trainPredict)= 0.9733800979123399
r2_score(y_val, valPredict)= 0.8791742142785777
r2_score(y_test, testPredict)= 0.8102707968443119
r2_score(combined_data, combined_datap)= 0.9597483634001194










from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Dense,add,Lambda,Flatten

# Define the first parallel recurrent layer with return_sequences=True
rnn_1 =LSTM(units=11)(input_1)
dropout1 = Dropout(rate=0.01)(rnn_1)

# Define the second parallel recurrent layer
rnn_2 = LSTM(units=11)(input_2)
dropout2 = Dropout(rate=0.01)(rnn_2)

# Define the third parallel recurrent layer
rnn_3 =LSTM(units=11)(input_3)
dropout3 = Dropout(rate=0.01)(rnn_3)

# Define the fourth parallel recurrent layer
rnn_4 = LSTM(units=11)(input_4)
dropout4 = Dropout(rate=0.01)(rnn_4)

# Concatenate the outputs of the recurrent layers
merged = concatenate([dropout1, dropout2, dropout3, dropout4], axis=1)



# Flatten the merged output
flatten = Flatten()(merged)

# ...
activation_layer = Activation('tanh')(flatten)
dense1 = Dense(units=50, activation='tanh')(activation_layer)#,kernel_regularizer=regularizers.l1(0.001))(activation_layer)
dense1 = BatchNormalization()(dense1)
dense2 = Dense(units=25, activation='relu')(dense1)# kernel_regularizer=regularizers.l1(0.001))(dense1)
dense2 = BatchNormalization()(dense2)



# Define the output layer
output = Dense(units=1, activation='sigmoid')(dense2)


# Create the model with the inputs and output
regressor = Model(inputs=[input_1, input_2,input_3, input_4], outputs=output)

# Compile the model with the desired optimizer, loss function, and metrics
regressor.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# Train the model with the training data
result=regressor.fit([X_train1, X_train2,X_train3, X_train4], y_train, epochs=1000, batch_size=25,validation_data=([X_val1, X_val2,X_val3, X_val4], y_val),callbacks=[LearningRateLogger()])
