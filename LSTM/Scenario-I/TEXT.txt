Case1:  all input features(F1, F2, F3, F4) allocated to each recurrent layer 




(X_train.shape[1], 1): (4,1)
input_shape =(X_train.shape[1], 1)
input = Input(shape=input_shape)

MSE=mean_squared_error(y_train,trainPredict)= 0.0008356270517836203
MSE=mean_squared_error(y_val,valPredict)= 0.001131066854379717
MSE=mean_squared_error(y_test,testPredict)= 0.008429630564412038

RMSE = (y_train,trainPredict)= 0.03
RMSE = (y_val,valPredict)= 0.03
RMSE = (y_test,testPredict)=  0.09

MAE =(y_train,trainPredict)= 0.15
MAE =(y_val, valPredict)=  0.17
MAE =(y_test,testPredict)= 0.29

r2_score(y_train, trainPredict)= 0.9873910630365326
r2_score(y_val, valPredict)= 0.960141930695234
r2_score(y_test, testPredict)= 0.47840913697578624
r2_score(combined_data, combined_datap)=0.9630568371752639



from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Dense,add, concatenate, Dropout, Reshape,Lambda,Flatten

# Define the first parallel recurrent layer with return_sequences=True
rnn_1 =LSTM(units=11)(input)
dropout1 = Dropout(rate=0.171)(rnn_1)


# Define the second parallel recurrent layer
rnn_2 = LSTM(units=11)(input)
dropout2 = Dropout(rate=0.171)(rnn_2)

# Define the third recurrent layer with return_sequences=True
rnn_3 =LSTM(units=11)(input)
dropout3 = Dropout(rate=0.171)(rnn_3)

# Define the fourthd parallel recurrent layer
rnn_4 = LSTM(units=11)(input)
dropout4 = Dropout(rate=0.171)(rnn_4)



# Concatenate the outputs of the recurrent layers
merged = concatenate([dropout1, dropout2,dropout3, dropout4], axis=1)

dense1 = Dense(units=25, activation='tanh', kernel_regularizer=regularizers.l1(0.001))(merged)
dense1 = BatchNormalization()(dense1)
dense2 = Dense(units=25, activation='relu', kernel_regularizer=regularizers.l1(0.001))(dense1)
dense2 = BatchNormalization()(dense2)

# Flatten the merged output
#flatten = Flatten()(merged)


# activation =Activation('tanh')(flatten)
# dense1 = Dense(units=50)(activation)
# activation2 = Activation('relu')(dense1)
#dense2 = Dense(units=1)(activation2)
#activation3 = Activation('sigmoid')(dense2)


# Define the output layer
output = Dense(units=1, activation='sigmoid')(dense2)

# Create the model with the inputs and output
model = Model(inputs=[input], outputs=output)

# Compile the model with the desired optimizer, loss function, and metrics
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# Train the model with the training data
model.fit([X_train], y_train, epochs=1000, batch_size=25, validation_data=([X_val], y_val), callbacks=[LearningRateLogger()])

